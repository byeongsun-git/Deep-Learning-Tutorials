{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 2016년에 가장 관심을 많이 받았던 비감독(Unsupervised) 학습 방법인\n",
    "# Generative Adversarial Network(GAN)을 구현\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "img_data = image.load_img('./cat_photos/chrtreux31.jpg', target_size=(64,64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.77254903, 0.78039217, 0.8627451 , ..., 0.13333334, 0.14509805,\n",
       "        0.17254902]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pix =np.array(img_data)\n",
    "pix =pix.astype('float32')\n",
    "pix /= 255.0\n",
    "print(pix.shape)\n",
    "pix = pix.reshape(-1,12288)\n",
    "pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# 옵션 설정\n",
    "#########\n",
    "total_epoch = 1000\n",
    "batch_size = 1\n",
    "learning_rate = 0.0001\n",
    "# 신경망 레이어 구성 옵션\n",
    "n_hidden = 256\n",
    "n_input = 12288\n",
    "n_noise = 64  # 생성기의 입력값으로 사용할 노이즈의 크기\n",
    "\n",
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "# GAN 도 Unsupervised 학습이므로 Autoencoder 처럼 Y 를 사용하지 않습니다.\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "# 노이즈 Z를 입력값으로 사용합니다.\n",
    "Z = tf.placeholder(tf.float32, [None, n_noise])\n",
    "\n",
    "# 생성기 신경망에 사용하는 변수들입니다.\n",
    "G_W1 = tf.Variable(tf.random_normal([n_noise, n_hidden], stddev=0.01))\n",
    "G_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "G_W2 = tf.Variable(tf.random_normal([n_hidden, n_input], stddev=0.01))\n",
    "G_b2 = tf.Variable(tf.zeros([n_input]))\n",
    "\n",
    "# 판별기 신경망에 사용하는 변수들입니다.\n",
    "D_W1 = tf.Variable(tf.random_normal([n_input, n_hidden], stddev=0.01))\n",
    "D_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "# 판별기의 최종 결과값은 얼마나 진짜와 가깝냐를 판단하는 한 개의 스칼라값입니다.\n",
    "D_W2 = tf.Variable(tf.random_normal([n_hidden, 1], stddev=0.01))\n",
    "D_b2 = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "\n",
    "# 생성기(G) 신경망을 구성합니다.\n",
    "def generator(noise_z):\n",
    "    hidden = tf.nn.relu(\n",
    "                    tf.matmul(noise_z, G_W1) + G_b1)\n",
    "    output = tf.nn.sigmoid(\n",
    "                    tf.matmul(hidden, G_W2) + G_b2)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# 판별기(D) 신경망을 구성합니다.\n",
    "def discriminator(inputs):\n",
    "    hidden = tf.nn.relu(\n",
    "                    tf.matmul(inputs, D_W1) + D_b1)\n",
    "    output = tf.nn.sigmoid(\n",
    "                    tf.matmul(hidden, D_W2) + D_b2)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# 랜덤한 노이즈(Z)를 만듭니다.\n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.normal(size=(batch_size, n_noise))\n",
    "\n",
    "\n",
    "# 노이즈를 이용해 랜덤한 이미지를 생성합니다.\n",
    "G = generator(Z)\n",
    "# 노이즈를 이용해 생성한 이미지가 진짜 이미지인지 판별한 값을 구합니다.\n",
    "D_gene = discriminator(G)\n",
    "# 진짜 이미지를 이용해 판별한 값을 구합니다.\n",
    "D_real = discriminator(X)\n",
    "\n",
    "# 논문에 따르면, GAN 모델의 최적화는 loss_G 와 loss_D 를 최대화 하는 것 입니다.\n",
    "# 다만 loss_D와 loss_G는 서로 연관관계가 있기 때문에 두 개의 손실값이 항상 같이 증가하는 경향을 보이지는 않을 것 입니다.\n",
    "# loss_D가 증가하려면 loss_G는 하락해야하고, loss_G가 증가하려면 loss_D는 하락해야하는 경쟁관계에 있기 때문입니다.\n",
    "# 논문의 수식에 따른 다음 로직을 보면 loss_D 를 최대화하기 위해서는 D_gene 값을 최소화하게 됩니다.\n",
    "# 판별기에 진짜 이미지를 넣었을 때에도 최대값을 : tf.log(D_real)\n",
    "# 가짜 이미지를 넣었을 때에도 최대값을 : tf.log(1 - D_gene)\n",
    "# 갖도록 학습시키기 때문입니다.\n",
    "# 이것은 판별기는 생성기가 만들어낸 이미지가 가짜라고 판단하도록 판별기 신경망을 학습시킵니다.\n",
    "loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1 - D_gene))\n",
    "# 반면 loss_G 를 최대화하기 위해서는 D_gene 값을 최대화하게 되는데,\n",
    "# 이것은 가짜 이미지를 넣었을 때, 판별기가 최대한 실제 이미지라고 판단하도록 생성기 신경망을 학습시킵니다.\n",
    "# 논문에서는 loss_D 와 같은 수식으로 최소화 하는 생성기를 찾지만,\n",
    "# 결국 D_gene 값을 최대화하는 것이므로 다음과 같이 사용할 수 있습니다.\n",
    "loss_G = tf.reduce_mean(tf.log(D_gene))\n",
    "\n",
    "# loss_D 를 구할 때는 판별기 신경망에 사용되는 변수만 사용하고,\n",
    "# loss_G 를 구할 때는 생성기 신경망에 사용되는 변수만 사용하여 최적화를 합니다.\n",
    "D_var_list = [D_W1, D_b1, D_W2, D_b2]\n",
    "G_var_list = [G_W1, G_b1, G_W2, G_b2]\n",
    "\n",
    "# GAN 논문의 수식에 따르면 loss 를 극대화 해야하지만, minimize 하는 최적화 함수를 사용하기 때문에\n",
    "# 최적화 하려는 loss_D 와 loss_G 에 음수 부호를 붙여줍니다.\n",
    "train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D,\n",
    "                                                         var_list=D_var_list)\n",
    "train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G,\n",
    "                                                         var_list=G_var_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 D loss: -1.401 G loss: -0.7157\n",
      "Epoch: 0001 D loss: -1.244 G loss: -0.7832\n",
      "Epoch: 0002 D loss: -1.121 G loss: -0.8653\n",
      "Epoch: 0003 D loss: -1.008 G loss: -0.9487\n",
      "Epoch: 0004 D loss: -0.9048 G loss: -1.033\n",
      "Epoch: 0005 D loss: -0.8083 G loss: -1.116\n",
      "Epoch: 0006 D loss: -0.7209 G loss: -1.202\n",
      "Epoch: 0007 D loss: -0.6415 G loss: -1.294\n",
      "Epoch: 0008 D loss: -0.5688 G loss: -1.397\n",
      "Epoch: 0009 D loss: -0.5025 G loss: -1.508\n",
      "Epoch: 0010 D loss: -0.4417 G loss: -1.634\n",
      "Epoch: 0011 D loss: -0.3888 G loss: -1.755\n",
      "Epoch: 0012 D loss: -0.3414 G loss: -1.875\n",
      "Epoch: 0013 D loss: -0.2997 G loss: -1.995\n",
      "Epoch: 0014 D loss: -0.2626 G loss: -2.111\n",
      "Epoch: 0015 D loss: -0.2309 G loss: -2.222\n",
      "Epoch: 0016 D loss: -0.202 G loss: -2.341\n",
      "Epoch: 0017 D loss: -0.1777 G loss: -2.457\n",
      "Epoch: 0018 D loss: -0.1574 G loss: -2.566\n",
      "Epoch: 0019 D loss: -0.1392 G loss: -2.676\n",
      "Epoch: 0020 D loss: -0.1238 G loss: -2.787\n",
      "Epoch: 0021 D loss: -0.11 G loss: -2.9\n",
      "Epoch: 0022 D loss: -0.09777 G loss: -3.012\n",
      "Epoch: 0023 D loss: -0.08799 G loss: -3.101\n",
      "Epoch: 0024 D loss: -0.07857 G loss: -3.211\n",
      "Epoch: 0025 D loss: -0.0711 G loss: -3.301\n",
      "Epoch: 0026 D loss: -0.06437 G loss: -3.389\n",
      "Epoch: 0027 D loss: -0.05794 G loss: -3.492\n",
      "Epoch: 0028 D loss: -0.05278 G loss: -3.58\n",
      "Epoch: 0029 D loss: -0.04872 G loss: -3.651\n",
      "Epoch: 0030 D loss: -0.04485 G loss: -3.731\n",
      "Epoch: 0031 D loss: -0.04136 G loss: -3.813\n",
      "Epoch: 0032 D loss: -0.03826 G loss: -3.893\n",
      "Epoch: 0033 D loss: -0.03531 G loss: -3.979\n",
      "Epoch: 0034 D loss: -0.03276 G loss: -4.06\n",
      "Epoch: 0035 D loss: -0.03057 G loss: -4.133\n",
      "Epoch: 0036 D loss: -0.02849 G loss: -4.21\n",
      "Epoch: 0037 D loss: -0.02651 G loss: -4.291\n",
      "Epoch: 0038 D loss: -0.02491 G loss: -4.357\n",
      "Epoch: 0039 D loss: -0.02433 G loss: -4.355\n",
      "Epoch: 0040 D loss: -0.02274 G loss: -4.434\n",
      "Epoch: 0041 D loss: -0.02173 G loss: -4.476\n",
      "Epoch: 0042 D loss: -0.02117 G loss: -4.487\n",
      "Epoch: 0043 D loss: -0.01972 G loss: -4.578\n",
      "Epoch: 0044 D loss: -0.01845 G loss: -4.662\n",
      "Epoch: 0045 D loss: -0.01819 G loss: -4.654\n",
      "Epoch: 0046 D loss: -0.01704 G loss: -4.737\n",
      "Epoch: 0047 D loss: -0.01662 G loss: -4.748\n",
      "Epoch: 0048 D loss: -0.01556 G loss: -4.834\n",
      "Epoch: 0049 D loss: -0.01543 G loss: -4.813\n",
      "Epoch: 0050 D loss: -0.01481 G loss: -4.854\n",
      "Epoch: 0051 D loss: -0.0139 G loss: -4.937\n",
      "Epoch: 0052 D loss: -0.01364 G loss: -4.939\n",
      "Epoch: 0053 D loss: -0.01354 G loss: -4.921\n",
      "Epoch: 0054 D loss: -0.01356 G loss: -4.892\n",
      "Epoch: 0055 D loss: -0.01315 G loss: -4.923\n",
      "Epoch: 0056 D loss: -0.01296 G loss: -4.928\n",
      "Epoch: 0057 D loss: -0.01214 G loss: -5.022\n",
      "Epoch: 0058 D loss: -0.01174 G loss: -5.062\n",
      "Epoch: 0059 D loss: -0.01163 G loss: -5.059\n",
      "Epoch: 0060 D loss: -0.01166 G loss: -5.038\n",
      "Epoch: 0061 D loss: -0.01068 G loss: -5.173\n",
      "Epoch: 0062 D loss: -0.01143 G loss: -5.04\n",
      "Epoch: 0063 D loss: -0.01025 G loss: -5.214\n",
      "Epoch: 0064 D loss: -0.01041 G loss: -5.168\n",
      "Epoch: 0065 D loss: -0.01017 G loss: -5.194\n",
      "Epoch: 0066 D loss: -0.01054 G loss: -5.119\n",
      "Epoch: 0067 D loss: -0.01016 G loss: -5.171\n",
      "Epoch: 0068 D loss: -0.0096 G loss: -5.257\n",
      "Epoch: 0069 D loss: -0.009747 G loss: -5.219\n",
      "Epoch: 0070 D loss: -0.009813 G loss: -5.198\n",
      "Epoch: 0071 D loss: -0.009355 G loss: -5.272\n",
      "Epoch: 0072 D loss: -0.008792 G loss: -5.369\n",
      "Epoch: 0073 D loss: -0.009127 G loss: -5.292\n",
      "Epoch: 0074 D loss: -0.008715 G loss: -5.362\n",
      "Epoch: 0075 D loss: -0.008704 G loss: -5.353\n",
      "Epoch: 0076 D loss: -0.008383 G loss: -5.406\n",
      "Epoch: 0077 D loss: -0.008213 G loss: -5.428\n",
      "Epoch: 0078 D loss: -0.007791 G loss: -5.507\n",
      "Epoch: 0079 D loss: -0.008344 G loss: -5.363\n",
      "Epoch: 0080 D loss: -0.008535 G loss: -5.311\n",
      "Epoch: 0081 D loss: -0.00788 G loss: -5.436\n",
      "Epoch: 0082 D loss: -0.007944 G loss: -5.41\n",
      "Epoch: 0083 D loss: -0.007783 G loss: -5.435\n",
      "Epoch: 0084 D loss: -0.007556 G loss: -5.476\n",
      "Epoch: 0085 D loss: -0.007886 G loss: -5.394\n",
      "Epoch: 0086 D loss: -0.007483 G loss: -5.476\n",
      "Epoch: 0087 D loss: -0.008049 G loss: -5.346\n",
      "Epoch: 0088 D loss: -0.007295 G loss: -5.507\n",
      "Epoch: 0089 D loss: -0.007556 G loss: -5.442\n",
      "Epoch: 0090 D loss: -0.007436 G loss: -5.465\n",
      "Epoch: 0091 D loss: -0.006957 G loss: -5.574\n",
      "Epoch: 0092 D loss: -0.007523 G loss: -5.436\n",
      "Epoch: 0093 D loss: -0.006696 G loss: -5.63\n",
      "Epoch: 0094 D loss: -0.00701 G loss: -5.543\n",
      "Epoch: 0095 D loss: -0.006588 G loss: -5.644\n",
      "Epoch: 0096 D loss: -0.006369 G loss: -5.694\n",
      "Epoch: 0097 D loss: -0.006354 G loss: -5.685\n",
      "Epoch: 0098 D loss: -0.007206 G loss: -5.453\n",
      "Epoch: 0099 D loss: -0.007449 G loss: -5.393\n",
      "Epoch: 0100 D loss: -0.007412 G loss: -5.397\n",
      "Epoch: 0101 D loss: -0.005922 G loss: -5.772\n",
      "Epoch: 0102 D loss: -0.006186 G loss: -5.684\n",
      "Epoch: 0103 D loss: -0.005952 G loss: -5.743\n",
      "Epoch: 0104 D loss: -0.005942 G loss: -5.736\n",
      "Epoch: 0105 D loss: -0.007038 G loss: -5.444\n",
      "Epoch: 0106 D loss: -0.005938 G loss: -5.723\n",
      "Epoch: 0107 D loss: -0.005835 G loss: -5.747\n",
      "Epoch: 0108 D loss: -0.006095 G loss: -5.665\n",
      "Epoch: 0109 D loss: -0.006419 G loss: -5.575\n",
      "Epoch: 0110 D loss: -0.006561 G loss: -5.538\n",
      "Epoch: 0111 D loss: -0.006215 G loss: -5.626\n",
      "Epoch: 0112 D loss: -0.005644 G loss: -5.79\n",
      "Epoch: 0113 D loss: -0.005544 G loss: -5.818\n",
      "Epoch: 0114 D loss: -0.005918 G loss: -5.701\n",
      "Epoch: 0115 D loss: -0.006835 G loss: -5.465\n",
      "Epoch: 0116 D loss: -0.005667 G loss: -5.776\n",
      "Epoch: 0117 D loss: -0.005664 G loss: -5.776\n",
      "Epoch: 0118 D loss: -0.005689 G loss: -5.767\n",
      "Epoch: 0119 D loss: -0.005729 G loss: -5.753\n",
      "Epoch: 0120 D loss: -0.005519 G loss: -5.816\n",
      "Epoch: 0121 D loss: -0.006491 G loss: -5.541\n",
      "Epoch: 0122 D loss: -0.005369 G loss: -5.863\n",
      "Epoch: 0123 D loss: -0.005021 G loss: -5.98\n",
      "Epoch: 0124 D loss: -0.004989 G loss: -5.983\n",
      "Epoch: 0125 D loss: -0.005557 G loss: -5.773\n",
      "Epoch: 0126 D loss: -0.004966 G loss: -5.968\n",
      "Epoch: 0127 D loss: -0.004688 G loss: -6.062\n",
      "Epoch: 0128 D loss: -0.005648 G loss: -5.705\n",
      "Epoch: 0129 D loss: -0.005948 G loss: -5.613\n",
      "Epoch: 0130 D loss: -0.005114 G loss: -5.858\n",
      "Epoch: 0131 D loss: -0.006067 G loss: -5.571\n",
      "Epoch: 0132 D loss: -0.004237 G loss: -6.194\n",
      "Epoch: 0133 D loss: -0.004894 G loss: -5.908\n",
      "Epoch: 0134 D loss: -0.004456 G loss: -6.067\n",
      "Epoch: 0135 D loss: -0.004577 G loss: -6.004\n",
      "Epoch: 0136 D loss: -0.007131 G loss: -5.304\n",
      "Epoch: 0137 D loss: -0.004423 G loss: -6.058\n",
      "Epoch: 0138 D loss: -0.006875 G loss: -5.357\n",
      "Epoch: 0139 D loss: -0.004681 G loss: -5.959\n",
      "Epoch: 0140 D loss: -0.004391 G loss: -6.075\n",
      "Epoch: 0141 D loss: -0.00469 G loss: -5.956\n",
      "Epoch: 0142 D loss: -0.003992 G loss: -6.25\n",
      "Epoch: 0143 D loss: -0.007018 G loss: -5.327\n",
      "Epoch: 0144 D loss: -0.004454 G loss: -6.046\n",
      "Epoch: 0145 D loss: -0.004279 G loss: -6.12\n",
      "Epoch: 0146 D loss: -0.004347 G loss: -6.088\n",
      "Epoch: 0147 D loss: -0.00561 G loss: -5.656\n",
      "Epoch: 0148 D loss: -0.004427 G loss: -6.056\n",
      "Epoch: 0149 D loss: -0.004321 G loss: -6.099\n",
      "Epoch: 0150 D loss: -0.004233 G loss: -6.134\n",
      "Epoch: 0151 D loss: -0.003966 G loss: -6.251\n",
      "Epoch: 0152 D loss: -0.006486 G loss: -5.43\n",
      "Epoch: 0153 D loss: -0.004199 G loss: -6.137\n",
      "Epoch: 0154 D loss: -0.004117 G loss: -6.17\n",
      "Epoch: 0155 D loss: -0.004408 G loss: -6.038\n",
      "Epoch: 0156 D loss: -0.003975 G loss: -6.225\n",
      "Epoch: 0157 D loss: -0.007004 G loss: -5.315\n",
      "Epoch: 0158 D loss: -0.004282 G loss: -6.087\n",
      "Epoch: 0159 D loss: -0.003804 G loss: -6.309\n",
      "Epoch: 0160 D loss: -0.004809 G loss: -5.886\n",
      "Epoch: 0161 D loss: -0.004184 G loss: -6.134\n",
      "Epoch: 0162 D loss: -0.004684 G loss: -5.935\n",
      "Epoch: 0163 D loss: -0.00367 G loss: -6.393\n",
      "Epoch: 0164 D loss: -0.003775 G loss: -6.324\n",
      "Epoch: 0165 D loss: -0.004256 G loss: -6.079\n",
      "Epoch: 0166 D loss: -0.005726 G loss: -5.592\n",
      "Epoch: 0167 D loss: -0.004277 G loss: -6.065\n",
      "Epoch: 0168 D loss: -0.004282 G loss: -6.059\n",
      "Epoch: 0169 D loss: -0.004911 G loss: -5.827\n",
      "Epoch: 0170 D loss: -0.004242 G loss: -6.075\n",
      "Epoch: 0171 D loss: -0.003303 G loss: -6.566\n",
      "Epoch: 0172 D loss: -0.003472 G loss: -6.439\n",
      "Epoch: 0173 D loss: -0.004426 G loss: -5.968\n",
      "Epoch: 0174 D loss: -0.004017 G loss: -6.127\n",
      "Epoch: 0175 D loss: -0.004211 G loss: -6.035\n",
      "Epoch: 0176 D loss: -0.005546 G loss: -5.604\n",
      "Epoch: 0177 D loss: -0.004336 G loss: -5.988\n",
      "Epoch: 0178 D loss: -0.004415 G loss: -5.961\n",
      "Epoch: 0179 D loss: -0.00652 G loss: -5.395\n",
      "Epoch: 0180 D loss: -0.003492 G loss: -6.417\n",
      "Epoch: 0181 D loss: -0.00475 G loss: -5.875\n",
      "Epoch: 0182 D loss: -0.005232 G loss: -5.736\n",
      "Epoch: 0183 D loss: -0.002974 G loss: -6.84\n",
      "Epoch: 0184 D loss: -0.003727 G loss: -6.33\n",
      "Epoch: 0185 D loss: -0.005107 G loss: -5.775\n",
      "Epoch: 0186 D loss: -0.003324 G loss: -6.58\n",
      "Epoch: 0187 D loss: -0.003146 G loss: -6.698\n",
      "Epoch: 0188 D loss: -0.004652 G loss: -5.905\n",
      "Epoch: 0189 D loss: -0.004841 G loss: -5.838\n",
      "Epoch: 0190 D loss: -0.003431 G loss: -6.462\n",
      "Epoch: 0191 D loss: -0.003726 G loss: -6.279\n",
      "Epoch: 0192 D loss: -0.003639 G loss: -6.309\n",
      "Epoch: 0193 D loss: -0.002877 G loss: -6.803\n",
      "Epoch: 0194 D loss: -0.004184 G loss: -6.016\n",
      "Epoch: 0195 D loss: -0.005206 G loss: -5.671\n",
      "Epoch: 0196 D loss: -0.003735 G loss: -6.201\n",
      "Epoch: 0197 D loss: -0.003375 G loss: -6.379\n",
      "Epoch: 0198 D loss: -0.002814 G loss: -6.745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0199 D loss: -0.004032 G loss: -6.034\n",
      "Epoch: 0200 D loss: -0.00324 G loss: -6.409\n",
      "Epoch: 0201 D loss: -0.004811 G loss: -5.75\n",
      "Epoch: 0202 D loss: -0.003914 G loss: -6.07\n",
      "Epoch: 0203 D loss: -0.003328 G loss: -6.348\n",
      "Epoch: 0204 D loss: -0.005418 G loss: -5.591\n",
      "Epoch: 0205 D loss: -0.003378 G loss: -6.339\n",
      "Epoch: 0206 D loss: -0.002729 G loss: -6.777\n",
      "Epoch: 0207 D loss: -0.006757 G loss: -5.314\n",
      "Epoch: 0208 D loss: -0.00511 G loss: -5.714\n",
      "Epoch: 0209 D loss: -0.002563 G loss: -7.044\n",
      "Epoch: 0210 D loss: -0.003106 G loss: -6.599\n",
      "Epoch: 0211 D loss: -0.00312 G loss: -6.594\n",
      "Epoch: 0212 D loss: -0.002877 G loss: -6.778\n",
      "Epoch: 0213 D loss: -0.002615 G loss: -7.009\n",
      "Epoch: 0214 D loss: -0.003324 G loss: -6.415\n",
      "Epoch: 0215 D loss: -0.003801 G loss: -6.148\n",
      "Epoch: 0216 D loss: -0.008151 G loss: -5.077\n",
      "Epoch: 0217 D loss: -0.002418 G loss: -7.177\n",
      "Epoch: 0218 D loss: -0.002439 G loss: -7.153\n",
      "Epoch: 0219 D loss: -0.002419 G loss: -7.155\n",
      "Epoch: 0220 D loss: -0.003819 G loss: -6.125\n",
      "Epoch: 0221 D loss: -0.002517 G loss: -6.97\n",
      "Epoch: 0222 D loss: -0.002593 G loss: -6.85\n",
      "Epoch: 0223 D loss: -0.002861 G loss: -6.588\n",
      "Epoch: 0224 D loss: -0.002844 G loss: -6.564\n",
      "Epoch: 0225 D loss: -0.006807 G loss: -5.248\n",
      "Epoch: 0226 D loss: -0.002376 G loss: -6.941\n",
      "Epoch: 0227 D loss: -0.00423 G loss: -5.891\n",
      "Epoch: 0228 D loss: -0.0294 G loss: -3.756\n",
      "Epoch: 0229 D loss: -0.00254 G loss: -7.155\n",
      "Epoch: 0230 D loss: -0.005701 G loss: -5.725\n",
      "Epoch: 0231 D loss: -0.006773 G loss: -5.537\n",
      "Epoch: 0232 D loss: -0.00342 G loss: -7.134\n",
      "Epoch: 0233 D loss: -0.00378 G loss: -7.04\n",
      "Epoch: 0234 D loss: -0.003843 G loss: -7.213\n",
      "Epoch: 0235 D loss: -0.0041 G loss: -7.069\n",
      "Epoch: 0236 D loss: -0.004679 G loss: -6.596\n",
      "Epoch: 0237 D loss: -0.007595 G loss: -5.472\n",
      "Epoch: 0238 D loss: -0.00431 G loss: -6.868\n",
      "Epoch: 0239 D loss: -0.003681 G loss: -7.581\n",
      "Epoch: 0240 D loss: -0.003806 G loss: -7.126\n",
      "Epoch: 0241 D loss: -0.003312 G loss: -7.582\n",
      "Epoch: 0242 D loss: -0.004201 G loss: -6.381\n",
      "Epoch: 0243 D loss: -0.006573 G loss: -5.439\n",
      "Epoch: 0244 D loss: -0.005232 G loss: -5.772\n",
      "Epoch: 0245 D loss: -0.01194 G loss: -4.645\n",
      "Epoch: 0246 D loss: -0.003055 G loss: -6.903\n",
      "Epoch: 0247 D loss: -0.002916 G loss: -6.996\n",
      "Epoch: 0248 D loss: -0.002973 G loss: -6.869\n",
      "Epoch: 0249 D loss: -0.004645 G loss: -5.889\n",
      "Epoch: 0250 D loss: -0.002287 G loss: -7.628\n",
      "Epoch: 0251 D loss: -0.002722 G loss: -6.897\n",
      "Epoch: 0252 D loss: -0.003585 G loss: -6.233\n",
      "Epoch: 0253 D loss: -0.003333 G loss: -6.329\n",
      "Epoch: 0254 D loss: -0.003935 G loss: -6.015\n",
      "Epoch: 0255 D loss: -0.003812 G loss: -6.049\n",
      "Epoch: 0256 D loss: -0.002609 G loss: -6.719\n",
      "Epoch: 0257 D loss: -0.004644 G loss: -5.731\n",
      "Epoch: 0258 D loss: -0.002951 G loss: -6.439\n",
      "Epoch: 0259 D loss: -0.001984 G loss: -7.307\n",
      "Epoch: 0260 D loss: -0.001629 G loss: -7.953\n",
      "Epoch: 0261 D loss: -0.1231 G loss: -2.679\n",
      "Epoch: 0262 D loss: -0.002861 G loss: -7.998\n",
      "Epoch: 0263 D loss: -0.006196 G loss: -6.694\n",
      "Epoch: 0264 D loss: -0.009318 G loss: -6.45\n",
      "Epoch: 0265 D loss: -0.01092 G loss: -8.155\n",
      "Epoch: 0266 D loss: -0.01449 G loss: -7.893\n",
      "Epoch: 0267 D loss: -0.01668 G loss: -7.911\n",
      "Epoch: 0268 D loss: -0.01668 G loss: -8.044\n",
      "Epoch: 0269 D loss: -0.01535 G loss: -6.878\n",
      "Epoch: 0270 D loss: -0.01211 G loss: -7.305\n",
      "Epoch: 0271 D loss: -0.008859 G loss: -8.124\n",
      "Epoch: 0272 D loss: -0.006447 G loss: -7.976\n",
      "Epoch: 0273 D loss: -0.00509 G loss: -6.958\n",
      "Epoch: 0274 D loss: -0.006938 G loss: -5.335\n",
      "Epoch: 0275 D loss: -0.002432 G loss: -8.3\n",
      "Epoch: 0276 D loss: -0.008779 G loss: -4.786\n",
      "Epoch: 0277 D loss: -0.002949 G loss: -6.252\n",
      "Epoch: 0278 D loss: -0.01441 G loss: -4.259\n",
      "Epoch: 0279 D loss: -0.001227 G loss: -7.994\n",
      "Epoch: 0280 D loss: -0.004353 G loss: -5.605\n",
      "Epoch: 0281 D loss: -0.001356 G loss: -7.343\n",
      "Epoch: 0282 D loss: -0.004284 G loss: -5.611\n",
      "Epoch: 0283 D loss: -0.002233 G loss: -6.434\n",
      "Epoch: 0284 D loss: -0.001118 G loss: -7.572\n",
      "Epoch: 0285 D loss: -0.004143 G loss: -5.65\n",
      "Epoch: 0286 D loss: -0.007031 G loss: -5.088\n",
      "Epoch: 0287 D loss: -0.00102 G loss: -7.753\n",
      "Epoch: 0288 D loss: -0.002774 G loss: -6.169\n",
      "Epoch: 0289 D loss: -0.003068 G loss: -6.054\n",
      "Epoch: 0290 D loss: -0.001153 G loss: -7.568\n",
      "Epoch: 0291 D loss: -0.01028 G loss: -4.736\n",
      "Epoch: 0292 D loss: -0.002497 G loss: -6.405\n",
      "Epoch: 0293 D loss: -0.002835 G loss: -6.26\n",
      "Epoch: 0294 D loss: -0.01867 G loss: -4.207\n",
      "Epoch: 0295 D loss: -0.003995 G loss: -5.976\n",
      "Epoch: 0296 D loss: -0.001588 G loss: -7.879\n",
      "Epoch: 0297 D loss: -0.006514 G loss: -5.426\n",
      "Epoch: 0298 D loss: -0.001841 G loss: -8.396\n",
      "Epoch: 0299 D loss: -0.003984 G loss: -6.259\n",
      "Epoch: 0300 D loss: -0.00726 G loss: -5.387\n",
      "Epoch: 0301 D loss: -0.002709 G loss: -7.971\n",
      "Epoch: 0302 D loss: -0.002845 G loss: -8.229\n",
      "Epoch: 0303 D loss: -0.003071 G loss: -7.999\n",
      "Epoch: 0304 D loss: -0.009103 G loss: -5.129\n",
      "Epoch: 0305 D loss: -0.003386 G loss: -7.733\n",
      "Epoch: 0306 D loss: -0.003694 G loss: -7.259\n",
      "Epoch: 0307 D loss: -0.01207 G loss: -4.749\n",
      "Epoch: 0308 D loss: -0.00326 G loss: -8.427\n",
      "Epoch: 0309 D loss: -0.004036 G loss: -6.914\n",
      "Epoch: 0310 D loss: -0.005343 G loss: -6.04\n",
      "Epoch: 0311 D loss: -0.003141 G loss: -8.188\n",
      "Epoch: 0312 D loss: -0.0104 G loss: -4.878\n",
      "Epoch: 0313 D loss: -0.0777 G loss: -3.008\n",
      "Epoch: 0314 D loss: -0.004757 G loss: -7.862\n",
      "Epoch: 0315 D loss: -0.007065 G loss: -7.359\n",
      "Epoch: 0316 D loss: -0.009962 G loss: -6.664\n",
      "Epoch: 0317 D loss: -0.01039 G loss: -9.16\n",
      "Epoch: 0318 D loss: -0.01168 G loss: -7.58\n",
      "Epoch: 0319 D loss: -0.01151 G loss: -7.264\n",
      "Epoch: 0320 D loss: -0.009694 G loss: -8.886\n",
      "Epoch: 0321 D loss: -0.008128 G loss: -7.884\n",
      "Epoch: 0322 D loss: -0.006371 G loss: -7.806\n",
      "Epoch: 0323 D loss: -0.07301 G loss: -2.884\n",
      "Epoch: 0324 D loss: -0.005588 G loss: -8.54\n",
      "Epoch: 0325 D loss: -0.1249 G loss: -2.806\n",
      "Epoch: 0326 D loss: -0.01428 G loss: -7.363\n",
      "Epoch: 0327 D loss: -0.02536 G loss: -7.075\n",
      "Epoch: 0328 D loss: -0.03561 G loss: -6.438\n",
      "Epoch: 0329 D loss: -0.03456 G loss: -9.733\n",
      "Epoch: 0330 D loss: -0.02611 G loss: -9.851\n",
      "Epoch: 0331 D loss: -0.01643 G loss: -7.785\n",
      "Epoch: 0332 D loss: -0.009344 G loss: -7.825\n",
      "Epoch: 0333 D loss: -0.005298 G loss: -7.631\n",
      "Epoch: 0334 D loss: -0.005086 G loss: -5.687\n",
      "Epoch: 0335 D loss: -0.676 G loss: -1.619\n",
      "Epoch: 0336 D loss: -0.008605 G loss: -9.201\n",
      "Epoch: 0337 D loss: -0.03592 G loss: -9.768\n",
      "Epoch: 0338 D loss: -0.1072 G loss: -10.66\n",
      "Epoch: 0339 D loss: -0.167 G loss: -10.91\n",
      "Epoch: 0340 D loss: -0.1206 G loss: -9.322\n",
      "Epoch: 0341 D loss: -0.05307 G loss: -8.887\n",
      "Epoch: 0342 D loss: -0.01953 G loss: -8.49\n",
      "Epoch: 0343 D loss: -0.01125 G loss: -4.811\n",
      "Epoch: 0344 D loss: -0.004059 G loss: -6.037\n",
      "Epoch: 0345 D loss: -0.004095 G loss: -5.292\n",
      "Epoch: 0346 D loss: -0.004664 G loss: -5.014\n",
      "Epoch: 0347 D loss: -0.009991 G loss: -4.241\n",
      "Epoch: 0348 D loss: -0.0004137 G loss: -8.093\n",
      "Epoch: 0349 D loss: -0.0007269 G loss: -7.128\n",
      "Epoch: 0350 D loss: -0.01031 G loss: -4.331\n",
      "Epoch: 0351 D loss: -0.1193 G loss: -2.303\n",
      "Epoch: 0352 D loss: -0.09179 G loss: -2.808\n",
      "Epoch: 0353 D loss: -0.2611 G loss: -2.341\n",
      "Epoch: 0354 D loss: -0.0004451 G loss: -9.013\n",
      "Epoch: 0355 D loss: -0.0009592 G loss: -8.536\n",
      "Epoch: 0356 D loss: -0.001895 G loss: -8.57\n",
      "Epoch: 0357 D loss: -0.005483 G loss: -7.021\n",
      "Epoch: 0358 D loss: -0.008436 G loss: -8.567\n",
      "Epoch: 0359 D loss: -0.01587 G loss: -9.463\n",
      "Epoch: 0360 D loss: -0.03394 G loss: -5.226\n",
      "Epoch: 0361 D loss: -0.0381 G loss: -9.427\n",
      "Epoch: 0362 D loss: -0.04503 G loss: -6.924\n",
      "Epoch: 0363 D loss: -0.04107 G loss: -9.481\n",
      "Epoch: 0364 D loss: -0.03235 G loss: -7.035\n",
      "Epoch: 0365 D loss: -0.02732 G loss: -4.861\n",
      "Epoch: 0366 D loss: -0.01412 G loss: -8.541\n",
      "Epoch: 0367 D loss: -0.008817 G loss: -10.67\n",
      "Epoch: 0368 D loss: -0.1705 G loss: -2.007\n",
      "Epoch: 0369 D loss: -0.009344 G loss: -5.924\n",
      "Epoch: 0370 D loss: -0.008096 G loss: -6.837\n",
      "Epoch: 0371 D loss: -0.008326 G loss: -6.955\n",
      "Epoch: 0372 D loss: -0.1271 G loss: -2.513\n",
      "Epoch: 0373 D loss: -0.01145 G loss: -8.747\n",
      "Epoch: 0374 D loss: -0.01564 G loss: -8.656\n",
      "Epoch: 0375 D loss: -0.01901 G loss: -10.32\n",
      "Epoch: 0376 D loss: -0.02123 G loss: -7.603\n",
      "Epoch: 0377 D loss: -0.02108 G loss: -7.004\n",
      "Epoch: 0378 D loss: -0.01792 G loss: -9.644\n",
      "Epoch: 0379 D loss: -0.01472 G loss: -8.744\n",
      "Epoch: 0380 D loss: -0.01995 G loss: -4.583\n",
      "Epoch: 0381 D loss: -0.008735 G loss: -9.664\n",
      "Epoch: 0382 D loss: -0.01103 G loss: -5.236\n",
      "Epoch: 0383 D loss: -0.01043 G loss: -5.059\n",
      "Epoch: 0384 D loss: -0.00588 G loss: -6.084\n",
      "Epoch: 0385 D loss: -0.003692 G loss: -7.324\n",
      "Epoch: 0386 D loss: -1.733 G loss: -0.7123\n",
      "Epoch: 0387 D loss: -0.01834 G loss: -10.88\n",
      "Epoch: 0388 D loss: -0.1008 G loss: -8.531\n",
      "Epoch: 0389 D loss: -0.3083 G loss: -11.77\n",
      "Epoch: 0390 D loss: -0.3584 G loss: -11.42\n",
      "Epoch: 0391 D loss: -0.179 G loss: -9.013\n",
      "Epoch: 0392 D loss: -0.0577 G loss: -6.669\n",
      "Epoch: 0393 D loss: -0.01751 G loss: -6.209\n",
      "Epoch: 0394 D loss: -0.1642 G loss: -1.503\n",
      "Epoch: 0395 D loss: -0.004642 G loss: -5.67\n",
      "Epoch: 0396 D loss: -0.002649 G loss: -6.189\n",
      "Epoch: 0397 D loss: -0.004512 G loss: -5.2\n",
      "Epoch: 0398 D loss: -0.01031 G loss: -4.307\n",
      "Epoch: 0399 D loss: -0.009358 G loss: -4.441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0400 D loss: -0.1048 G loss: -2.269\n",
      "Epoch: 0401 D loss: -0.08038 G loss: -2.668\n",
      "Epoch: 0402 D loss: -0.01517 G loss: -4.343\n",
      "Epoch: 0403 D loss: -0.001057 G loss: -7.196\n",
      "Epoch: 0404 D loss: -0.03906 G loss: -3.447\n",
      "Epoch: 0405 D loss: -0.003647 G loss: -5.873\n",
      "Epoch: 0406 D loss: -0.0003518 G loss: -10.15\n",
      "Epoch: 0407 D loss: -0.000503 G loss: -8.876\n",
      "Epoch: 0408 D loss: -0.003824 G loss: -5.814\n",
      "Epoch: 0409 D loss: -0.01136 G loss: -4.665\n",
      "Epoch: 0410 D loss: -0.004471 G loss: -5.67\n",
      "Epoch: 0411 D loss: -0.9092 G loss: -1.129\n",
      "Epoch: 0412 D loss: -0.2193 G loss: -2.827\n",
      "Epoch: 0413 D loss: -0.0128 G loss: -10.32\n",
      "Epoch: 0414 D loss: -0.055 G loss: -11.42\n",
      "Epoch: 0415 D loss: -0.1699 G loss: -7.68\n",
      "Epoch: 0416 D loss: -0.2923 G loss: -7.757\n",
      "Epoch: 0417 D loss: -2.811 G loss: -0.1756\n",
      "Epoch: 0418 D loss: -0.5791 G loss: -5.584\n",
      "Epoch: 0419 D loss: -0.4932 G loss: -9.478\n",
      "Epoch: 0420 D loss: -0.1991 G loss: -7.157\n",
      "Epoch: 0421 D loss: -0.05506 G loss: -8.081\n",
      "Epoch: 0422 D loss: -0.02624 G loss: -3.579\n",
      "Epoch: 0423 D loss: -0.01148 G loss: -4.139\n",
      "Epoch: 0424 D loss: -0.001584 G loss: -8.44\n",
      "Epoch: 0425 D loss: -2.404 G loss: -0.1536\n",
      "Epoch: 0426 D loss: -0.00136 G loss: -8.092\n",
      "Epoch: 0427 D loss: -0.1982 G loss: -2.268\n",
      "Epoch: 0428 D loss: -0.2062 G loss: -2.373\n",
      "Epoch: 0429 D loss: -0.008736 G loss: -11.15\n",
      "Epoch: 0430 D loss: -0.02639 G loss: -5.628\n",
      "Epoch: 0431 D loss: -0.04032 G loss: -9.751\n",
      "Epoch: 0432 D loss: -0.06997 G loss: -8.967\n",
      "Epoch: 0433 D loss: -0.1006 G loss: -7.155\n",
      "Epoch: 0434 D loss: -0.1179 G loss: -5.59\n",
      "Epoch: 0435 D loss: -0.1062 G loss: -6.903\n",
      "Epoch: 0436 D loss: -0.08423 G loss: -5.375\n",
      "Epoch: 0437 D loss: -0.05507 G loss: -7.38\n",
      "Epoch: 0438 D loss: -0.03966 G loss: -4.906\n",
      "Epoch: 0439 D loss: -0.02274 G loss: -6.248\n",
      "Epoch: 0440 D loss: -0.01375 G loss: -7.355\n",
      "Epoch: 0441 D loss: -0.008573 G loss: -8.393\n",
      "Epoch: 0442 D loss: -0.00585 G loss: -7.682\n",
      "Epoch: 0443 D loss: -2.128 G loss: -0.2433\n",
      "Epoch: 0444 D loss: -0.009814 G loss: -7.392\n",
      "Epoch: 0445 D loss: -0.01949 G loss: -9.553\n",
      "Epoch: 0446 D loss: -0.04135 G loss: -5.997\n",
      "Epoch: 0447 D loss: -0.06394 G loss: -8.995\n",
      "Epoch: 0448 D loss: -0.09174 G loss: -7.309\n",
      "Epoch: 0449 D loss: -2.632 G loss: -0.1942\n",
      "Epoch: 0450 D loss: -0.3073 G loss: -6.02\n",
      "Epoch: 0451 D loss: -0.4833 G loss: -9.087\n",
      "Epoch: 0452 D loss: -0.5373 G loss: -1.864\n",
      "Epoch: 0453 D loss: -0.2524 G loss: -6.837\n",
      "Epoch: 0454 D loss: -0.1087 G loss: -7.449\n",
      "Epoch: 0455 D loss: -0.04147 G loss: -8.281\n",
      "Epoch: 0456 D loss: -0.05109 G loss: -2.777\n",
      "Epoch: 0457 D loss: -2.3 G loss: -0.1406\n",
      "Epoch: 0458 D loss: -0.01977 G loss: -4.823\n",
      "Epoch: 0459 D loss: -0.2142 G loss: -2.028\n",
      "Epoch: 0460 D loss: -0.01995 G loss: -7.641\n",
      "Epoch: 0461 D loss: -0.1345 G loss: -2.62\n",
      "Epoch: 0462 D loss: -0.04337 G loss: -7.034\n",
      "Epoch: 0463 D loss: -0.4955 G loss: -1.384\n",
      "Epoch: 0464 D loss: -0.3154 G loss: -2.08\n",
      "Epoch: 0465 D loss: -0.1835 G loss: -6.611\n",
      "Epoch: 0466 D loss: -0.2376 G loss: -6.392\n",
      "Epoch: 0467 D loss: -0.2206 G loss: -7.837\n",
      "Epoch: 0468 D loss: -0.1554 G loss: -8.079\n",
      "Epoch: 0469 D loss: -0.09075 G loss: -7.283\n",
      "Epoch: 0470 D loss: -0.09321 G loss: -2.711\n",
      "Epoch: 0471 D loss: -0.06832 G loss: -2.827\n",
      "Epoch: 0472 D loss: -0.03945 G loss: -3.403\n",
      "Epoch: 0473 D loss: -0.2388 G loss: -1.489\n",
      "Epoch: 0474 D loss: -0.007899 G loss: -8.196\n",
      "Epoch: 0475 D loss: -0.9112 G loss: -0.7105\n",
      "Epoch: 0476 D loss: -0.0279 G loss: -4.442\n",
      "Epoch: 0477 D loss: -0.07976 G loss: -3.161\n",
      "Epoch: 0478 D loss: -5.458 G loss: -0.009548\n",
      "Epoch: 0479 D loss: -0.07745 G loss: -7.452\n",
      "Epoch: 0480 D loss: -0.1803 G loss: -5.681\n",
      "Epoch: 0481 D loss: -0.2865 G loss: -8.531\n",
      "Epoch: 0482 D loss: -0.3143 G loss: -8.4\n",
      "Epoch: 0483 D loss: -0.2696 G loss: -3.152\n",
      "Epoch: 0484 D loss: -0.1438 G loss: -7.529\n",
      "Epoch: 0485 D loss: -0.2494 G loss: -1.512\n",
      "Epoch: 0486 D loss: -0.1027 G loss: -2.539\n",
      "Epoch: 0487 D loss: -0.08369 G loss: -2.682\n",
      "Epoch: 0488 D loss: -0.02543 G loss: -5.958\n",
      "Epoch: 0489 D loss: -0.02238 G loss: -5.288\n",
      "Epoch: 0490 D loss: -4.995 G loss: -0.01027\n",
      "Epoch: 0491 D loss: -1.608 G loss: -0.4338\n",
      "Epoch: 0492 D loss: -1.066 G loss: -0.9209\n",
      "Epoch: 0493 D loss: -0.2953 G loss: -2.654\n",
      "Epoch: 0494 D loss: -0.4388 G loss: -8.359\n",
      "Epoch: 0495 D loss: -0.8112 G loss: -2.113\n",
      "Epoch: 0496 D loss: -0.7296 G loss: -3.977\n",
      "Epoch: 0497 D loss: -2.721 G loss: -0.1172\n",
      "Epoch: 0498 D loss: -1.433 G loss: -0.4881\n",
      "Epoch: 0499 D loss: -0.4455 G loss: -3.726\n",
      "Epoch: 0500 D loss: -0.3248 G loss: -3.888\n",
      "Epoch: 0501 D loss: -0.1926 G loss: -7.479\n",
      "Epoch: 0502 D loss: -0.1163 G loss: -4.638\n",
      "Epoch: 0503 D loss: -0.2966 G loss: -1.295\n",
      "Epoch: 0504 D loss: -0.04259 G loss: -4.95\n",
      "Epoch: 0505 D loss: -0.2035 G loss: -1.64\n",
      "Epoch: 0506 D loss: -0.6883 G loss: -0.7654\n",
      "Epoch: 0507 D loss: -0.1858 G loss: -2.022\n",
      "Epoch: 0508 D loss: -1.248 G loss: -0.5094\n",
      "Epoch: 0509 D loss: -0.5167 G loss: -1.389\n",
      "Epoch: 0510 D loss: -0.05565 G loss: -7.247\n",
      "Epoch: 0511 D loss: -0.1095 G loss: -5.24\n",
      "Epoch: 0512 D loss: -0.1957 G loss: -3.702\n",
      "Epoch: 0513 D loss: -0.2355 G loss: -4.463\n",
      "Epoch: 0514 D loss: -0.329 G loss: -2.494\n",
      "Epoch: 0515 D loss: -0.2282 G loss: -7.359\n",
      "Epoch: 0516 D loss: -0.3612 G loss: -1.662\n",
      "Epoch: 0517 D loss: -2.572 G loss: -0.1113\n",
      "Epoch: 0518 D loss: -0.1613 G loss: -7.322\n",
      "Epoch: 0519 D loss: -0.1624 G loss: -8.226\n",
      "Epoch: 0520 D loss: -0.1572 G loss: -4.367\n",
      "Epoch: 0521 D loss: -0.1913 G loss: -2.546\n",
      "Epoch: 0522 D loss: -0.09816 G loss: -6.37\n",
      "Epoch: 0523 D loss: -0.08364 G loss: -4.475\n",
      "Epoch: 0524 D loss: -0.05478 G loss: -7.422\n",
      "Epoch: 0525 D loss: -1.959 G loss: -0.1875\n",
      "Epoch: 0526 D loss: -1.727 G loss: -0.3189\n",
      "Epoch: 0527 D loss: -0.1019 G loss: -4.288\n",
      "Epoch: 0528 D loss: -0.322 G loss: -2.087\n",
      "Epoch: 0529 D loss: -0.2568 G loss: -3.084\n",
      "Epoch: 0530 D loss: -0.2571 G loss: -7.174\n",
      "Epoch: 0531 D loss: -0.3401 G loss: -2.56\n",
      "Epoch: 0532 D loss: -0.2394 G loss: -9.359\n",
      "Epoch: 0533 D loss: -0.1869 G loss: -6.382\n",
      "Epoch: 0534 D loss: -0.2043 G loss: -2.345\n",
      "Epoch: 0535 D loss: -0.08791 G loss: -6.735\n",
      "Epoch: 0536 D loss: -0.1126 G loss: -2.643\n",
      "Epoch: 0537 D loss: -0.0581 G loss: -3.768\n",
      "Epoch: 0538 D loss: -0.04294 G loss: -3.912\n",
      "Epoch: 0539 D loss: -1.397 G loss: -0.3183\n",
      "Epoch: 0540 D loss: -0.4115 G loss: -1.329\n",
      "Epoch: 0541 D loss: -0.1534 G loss: -2.417\n",
      "Epoch: 0542 D loss: -0.05031 G loss: -4.48\n",
      "Epoch: 0543 D loss: -0.1297 G loss: -2.801\n",
      "Epoch: 0544 D loss: -0.2194 G loss: -2.175\n",
      "Epoch: 0545 D loss: -0.08672 G loss: -6.414\n",
      "Epoch: 0546 D loss: -0.1089 G loss: -5.0\n",
      "Epoch: 0547 D loss: -2.018 G loss: -0.2261\n",
      "Epoch: 0548 D loss: -0.1751 G loss: -6.377\n",
      "Epoch: 0549 D loss: -0.738 G loss: -1.079\n",
      "Epoch: 0550 D loss: -0.4019 G loss: -2.269\n",
      "Epoch: 0551 D loss: -0.4747 G loss: -1.896\n",
      "Epoch: 0552 D loss: -0.3313 G loss: -3.591\n",
      "Epoch: 0553 D loss: -0.2512 G loss: -6.724\n",
      "Epoch: 0554 D loss: -0.2259 G loss: -2.736\n",
      "Epoch: 0555 D loss: -0.1287 G loss: -4.391\n",
      "Epoch: 0556 D loss: -0.07768 G loss: -5.386\n",
      "Epoch: 0557 D loss: -0.3655 G loss: -1.154\n",
      "Epoch: 0558 D loss: -0.0903 G loss: -2.711\n",
      "Epoch: 0559 D loss: -0.05303 G loss: -3.441\n",
      "Epoch: 0560 D loss: -0.2263 G loss: -1.65\n",
      "Epoch: 0561 D loss: -1.329 G loss: -0.4017\n",
      "Epoch: 0562 D loss: -0.03599 G loss: -4.679\n",
      "Epoch: 0563 D loss: -0.2231 G loss: -2.041\n",
      "Epoch: 0564 D loss: -0.0504 G loss: -8.029\n",
      "Epoch: 0565 D loss: -0.08024 G loss: -4.451\n",
      "Epoch: 0566 D loss: -0.1136 G loss: -3.619\n",
      "Epoch: 0567 D loss: -0.3422 G loss: -1.658\n",
      "Epoch: 0568 D loss: -0.1319 G loss: -4.168\n",
      "Epoch: 0569 D loss: -0.1247 G loss: -8.33\n",
      "Epoch: 0570 D loss: -0.1273 G loss: -4.732\n",
      "Epoch: 0571 D loss: -0.3672 G loss: -1.439\n",
      "Epoch: 0572 D loss: -0.191 G loss: -2.366\n",
      "Epoch: 0573 D loss: -1.668 G loss: -0.3005\n",
      "Epoch: 0574 D loss: -0.1482 G loss: -3.911\n",
      "Epoch: 0575 D loss: -0.1535 G loss: -6.184\n",
      "Epoch: 0576 D loss: -0.1763 G loss: -4.03\n",
      "Epoch: 0577 D loss: -0.6391 G loss: -1.009\n",
      "Epoch: 0578 D loss: -0.1668 G loss: -5.77\n",
      "Epoch: 0579 D loss: -0.5143 G loss: -1.227\n",
      "Epoch: 0580 D loss: -0.1632 G loss: -5.247\n",
      "Epoch: 0581 D loss: -0.1421 G loss: -7.582\n",
      "Epoch: 0582 D loss: -0.1934 G loss: -2.401\n",
      "Epoch: 0583 D loss: -0.6143 G loss: -0.8978\n",
      "Epoch: 0584 D loss: -0.09514 G loss: -4.822\n",
      "Epoch: 0585 D loss: -0.3423 G loss: -1.48\n",
      "Epoch: 0586 D loss: -0.08619 G loss: -4.914\n",
      "Epoch: 0587 D loss: -2.335 G loss: -0.156\n",
      "Epoch: 0588 D loss: -0.1166 G loss: -6.226\n",
      "Epoch: 0589 D loss: -0.1636 G loss: -4.7\n",
      "Epoch: 0590 D loss: -0.1794 G loss: -6.225\n",
      "Epoch: 0591 D loss: -0.1767 G loss: -6.58\n",
      "Epoch: 0592 D loss: -0.154 G loss: -5.756\n",
      "Epoch: 0593 D loss: -0.2679 G loss: -1.82\n",
      "Epoch: 0594 D loss: -1.649 G loss: -0.2807\n",
      "Epoch: 0595 D loss: -1.074 G loss: -0.6593\n",
      "Epoch: 0596 D loss: -0.5501 G loss: -1.48\n",
      "Epoch: 0597 D loss: -0.2888 G loss: -6.102\n",
      "Epoch: 0598 D loss: -0.4231 G loss: -2.658\n",
      "Epoch: 0599 D loss: -1.99 G loss: -0.2648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0600 D loss: -0.4329 G loss: -4.411\n",
      "Epoch: 0601 D loss: -0.3926 G loss: -5.339\n",
      "Epoch: 0602 D loss: -0.3845 G loss: -2.012\n",
      "Epoch: 0603 D loss: -0.9678 G loss: -0.5504\n",
      "Epoch: 0604 D loss: -0.3089 G loss: -1.763\n",
      "Epoch: 0605 D loss: -0.1214 G loss: -5.079\n",
      "Epoch: 0606 D loss: -0.1056 G loss: -3.707\n",
      "Epoch: 0607 D loss: -0.4262 G loss: -1.129\n",
      "Epoch: 0608 D loss: -0.5772 G loss: -0.9585\n",
      "Epoch: 0609 D loss: -0.1307 G loss: -2.756\n",
      "Epoch: 0610 D loss: -0.917 G loss: -0.7081\n",
      "Epoch: 0611 D loss: -0.4091 G loss: -1.601\n",
      "Epoch: 0612 D loss: -0.1744 G loss: -3.846\n",
      "Epoch: 0613 D loss: -0.6742 G loss: -1.172\n",
      "Epoch: 0614 D loss: -0.9828 G loss: -0.8801\n",
      "Epoch: 0615 D loss: -0.7538 G loss: -1.409\n",
      "Epoch: 0616 D loss: -1.358 G loss: -0.6798\n",
      "Epoch: 0617 D loss: -0.677 G loss: -2.949\n",
      "Epoch: 0618 D loss: -0.5744 G loss: -3.849\n",
      "Epoch: 0619 D loss: -0.9219 G loss: -0.6962\n",
      "Epoch: 0620 D loss: -0.2618 G loss: -3.605\n",
      "Epoch: 0621 D loss: -0.7495 G loss: -0.6705\n",
      "Epoch: 0622 D loss: -1.0 G loss: -0.5384\n",
      "Epoch: 0623 D loss: -0.2205 G loss: -2.189\n",
      "Epoch: 0624 D loss: -0.1392 G loss: -3.171\n",
      "Epoch: 0625 D loss: -0.5111 G loss: -1.138\n",
      "Epoch: 0626 D loss: -0.1075 G loss: -4.773\n",
      "Epoch: 0627 D loss: -0.3046 G loss: -1.746\n",
      "Epoch: 0628 D loss: -0.1352 G loss: -3.478\n",
      "Epoch: 0629 D loss: -1.083 G loss: -0.5781\n",
      "Epoch: 0630 D loss: -0.2021 G loss: -2.933\n",
      "Epoch: 0631 D loss: -0.4845 G loss: -1.451\n",
      "Epoch: 0632 D loss: -0.3018 G loss: -2.47\n",
      "Epoch: 0633 D loss: -0.4863 G loss: -1.525\n",
      "Epoch: 0634 D loss: -0.5121 G loss: -1.46\n",
      "Epoch: 0635 D loss: -0.794 G loss: -0.9315\n",
      "Epoch: 0636 D loss: -1.307 G loss: -0.5508\n",
      "Epoch: 0637 D loss: -1.051 G loss: -0.8629\n",
      "Epoch: 0638 D loss: -0.4959 G loss: -3.976\n",
      "Epoch: 0639 D loss: -0.5071 G loss: -2.976\n",
      "Epoch: 0640 D loss: -0.4092 G loss: -2.463\n",
      "Epoch: 0641 D loss: -0.2453 G loss: -3.015\n",
      "Epoch: 0642 D loss: -0.1184 G loss: -5.689\n",
      "Epoch: 0643 D loss: -0.4527 G loss: -0.9196\n",
      "Epoch: 0644 D loss: -0.3513 G loss: -1.212\n",
      "Epoch: 0645 D loss: -0.2444 G loss: -1.632\n",
      "Epoch: 0646 D loss: -1.906 G loss: -0.2355\n",
      "Epoch: 0647 D loss: -1.591 G loss: -0.444\n",
      "Epoch: 0648 D loss: -0.7654 G loss: -1.25\n",
      "Epoch: 0649 D loss: -0.7177 G loss: -1.622\n",
      "Epoch: 0650 D loss: -0.6392 G loss: -4.754\n",
      "Epoch: 0651 D loss: -1.237 G loss: -1.154\n",
      "Epoch: 0652 D loss: -0.8725 G loss: -3.912\n",
      "Epoch: 0653 D loss: -0.7899 G loss: -1.485\n",
      "Epoch: 0654 D loss: -0.6745 G loss: -0.9517\n",
      "Epoch: 0655 D loss: -1.054 G loss: -0.4417\n",
      "Epoch: 0656 D loss: -0.3713 G loss: -1.347\n",
      "Epoch: 0657 D loss: -0.9886 G loss: -0.5461\n",
      "Epoch: 0658 D loss: -0.505 G loss: -1.192\n",
      "Epoch: 0659 D loss: -0.705 G loss: -0.983\n",
      "Epoch: 0660 D loss: -1.016 G loss: -0.78\n",
      "Epoch: 0661 D loss: -0.7943 G loss: -1.198\n",
      "Epoch: 0662 D loss: -0.6006 G loss: -2.098\n",
      "Epoch: 0663 D loss: -1.038 G loss: -1.077\n",
      "Epoch: 0664 D loss: -0.6428 G loss: -4.374\n",
      "Epoch: 0665 D loss: -1.302 G loss: -0.5554\n",
      "Epoch: 0666 D loss: -0.486 G loss: -2.391\n",
      "Epoch: 0667 D loss: -0.2827 G loss: -4.612\n",
      "Epoch: 0668 D loss: -0.1556 G loss: -5.302\n",
      "Epoch: 0669 D loss: -1.212 G loss: -0.343\n",
      "Epoch: 0670 D loss: -0.4577 G loss: -1.103\n",
      "Epoch: 0671 D loss: -0.4867 G loss: -1.143\n",
      "Epoch: 0672 D loss: -0.3393 G loss: -1.595\n",
      "Epoch: 0673 D loss: -0.1038 G loss: -3.989\n",
      "Epoch: 0674 D loss: -0.2184 G loss: -2.28\n",
      "Epoch: 0675 D loss: -0.7489 G loss: -0.9187\n",
      "Epoch: 0676 D loss: -0.7117 G loss: -1.094\n",
      "Epoch: 0677 D loss: -0.3958 G loss: -2.18\n",
      "Epoch: 0678 D loss: -0.3862 G loss: -2.865\n",
      "Epoch: 0679 D loss: -0.6426 G loss: -1.388\n",
      "Epoch: 0680 D loss: -0.6153 G loss: -1.399\n",
      "Epoch: 0681 D loss: -1.113 G loss: -0.626\n",
      "Epoch: 0682 D loss: -1.177 G loss: -0.628\n",
      "Epoch: 0683 D loss: -0.5289 G loss: -1.888\n",
      "Epoch: 0684 D loss: -0.5456 G loss: -1.662\n",
      "Epoch: 0685 D loss: -0.5683 G loss: -1.345\n",
      "Epoch: 0686 D loss: -0.393 G loss: -1.795\n",
      "Epoch: 0687 D loss: -0.6729 G loss: -0.8886\n",
      "Epoch: 0688 D loss: -0.313 G loss: -1.823\n",
      "Epoch: 0689 D loss: -0.8747 G loss: -0.6934\n",
      "Epoch: 0690 D loss: -0.2624 G loss: -2.221\n",
      "Epoch: 0691 D loss: -0.6996 G loss: -0.9701\n",
      "Epoch: 0692 D loss: -0.4528 G loss: -1.574\n",
      "Epoch: 0693 D loss: -1.873 G loss: -0.3223\n",
      "Epoch: 0694 D loss: -0.8367 G loss: -1.231\n",
      "Epoch: 0695 D loss: -1.288 G loss: -0.8258\n",
      "Epoch: 0696 D loss: -0.9151 G loss: -1.778\n",
      "Epoch: 0697 D loss: -1.421 G loss: -0.6974\n",
      "Epoch: 0698 D loss: -1.137 G loss: -0.8886\n",
      "Epoch: 0699 D loss: -0.8156 G loss: -1.159\n",
      "Epoch: 0700 D loss: -0.5443 G loss: -1.414\n",
      "Epoch: 0701 D loss: -0.6564 G loss: -0.8313\n",
      "Epoch: 0702 D loss: -0.4443 G loss: -1.159\n",
      "Epoch: 0703 D loss: -0.8818 G loss: -0.6281\n",
      "Epoch: 0704 D loss: -1.465 G loss: -0.4091\n",
      "Epoch: 0705 D loss: -1.008 G loss: -0.8275\n",
      "Epoch: 0706 D loss: -0.5571 G loss: -1.761\n",
      "Epoch: 0707 D loss: -0.7609 G loss: -1.525\n",
      "Epoch: 0708 D loss: -0.705 G loss: -2.176\n",
      "Epoch: 0709 D loss: -1.129 G loss: -0.8502\n",
      "Epoch: 0710 D loss: -0.7669 G loss: -1.38\n",
      "Epoch: 0711 D loss: -0.9007 G loss: -0.8128\n",
      "Epoch: 0712 D loss: -0.9732 G loss: -0.665\n",
      "Epoch: 0713 D loss: -0.5681 G loss: -1.212\n",
      "Epoch: 0714 D loss: -0.9992 G loss: -0.6437\n",
      "Epoch: 0715 D loss: -0.968 G loss: -0.7529\n",
      "Epoch: 0716 D loss: -0.4188 G loss: -2.021\n",
      "Epoch: 0717 D loss: -1.195 G loss: -0.6395\n",
      "Epoch: 0718 D loss: -0.8477 G loss: -1.096\n",
      "Epoch: 0719 D loss: -1.009 G loss: -0.949\n",
      "Epoch: 0720 D loss: -0.7956 G loss: -1.385\n",
      "Epoch: 0721 D loss: -1.005 G loss: -0.9162\n",
      "Epoch: 0722 D loss: -0.8165 G loss: -1.102\n",
      "Epoch: 0723 D loss: -0.9037 G loss: -0.8385\n",
      "Epoch: 0724 D loss: -1.019 G loss: -0.6987\n",
      "Epoch: 0725 D loss: -0.536 G loss: -1.459\n",
      "Epoch: 0726 D loss: -0.9174 G loss: -0.7615\n",
      "Epoch: 0727 D loss: -0.8926 G loss: -0.8388\n",
      "Epoch: 0728 D loss: -0.6956 G loss: -1.182\n",
      "Epoch: 0729 D loss: -0.7288 G loss: -1.166\n",
      "Epoch: 0730 D loss: -0.5457 G loss: -1.684\n",
      "Epoch: 0731 D loss: -0.4785 G loss: -1.852\n",
      "Epoch: 0732 D loss: -0.8712 G loss: -0.7944\n",
      "Epoch: 0733 D loss: -0.8025 G loss: -0.9189\n",
      "Epoch: 0734 D loss: -0.829 G loss: -0.9112\n",
      "Epoch: 0735 D loss: -0.7572 G loss: -1.065\n",
      "Epoch: 0736 D loss: -0.6567 G loss: -1.312\n",
      "Epoch: 0737 D loss: -0.8535 G loss: -1.004\n",
      "Epoch: 0738 D loss: -0.5849 G loss: -1.613\n",
      "Epoch: 0739 D loss: -0.7755 G loss: -1.04\n",
      "Epoch: 0740 D loss: -0.6859 G loss: -1.147\n",
      "Epoch: 0741 D loss: -0.8265 G loss: -0.9018\n",
      "Epoch: 0742 D loss: -0.5846 G loss: -1.314\n",
      "Epoch: 0743 D loss: -0.8788 G loss: -0.8337\n",
      "Epoch: 0744 D loss: -0.6872 G loss: -1.141\n",
      "Epoch: 0745 D loss: -0.8357 G loss: -0.9606\n",
      "Epoch: 0746 D loss: -0.7434 G loss: -1.157\n",
      "Epoch: 0747 D loss: -0.7232 G loss: -1.228\n",
      "Epoch: 0748 D loss: -0.8094 G loss: -1.079\n",
      "Epoch: 0749 D loss: -0.7751 G loss: -1.084\n",
      "Epoch: 0750 D loss: -0.8333 G loss: -0.9633\n",
      "Epoch: 0751 D loss: -0.7395 G loss: -1.092\n",
      "Epoch: 0752 D loss: -0.8414 G loss: -0.9173\n",
      "Epoch: 0753 D loss: -0.8559 G loss: -0.9271\n",
      "Epoch: 0754 D loss: -0.8466 G loss: -0.9629\n",
      "Epoch: 0755 D loss: -0.8992 G loss: -0.9354\n",
      "Epoch: 0756 D loss: -0.8074 G loss: -1.101\n",
      "Epoch: 0757 D loss: -0.8156 G loss: -1.083\n",
      "Epoch: 0758 D loss: -0.7837 G loss: -1.102\n",
      "Epoch: 0759 D loss: -0.7989 G loss: -1.015\n",
      "Epoch: 0760 D loss: -0.8888 G loss: -0.863\n",
      "Epoch: 0761 D loss: -0.8751 G loss: -0.8934\n",
      "Epoch: 0762 D loss: -0.8752 G loss: -0.9277\n",
      "Epoch: 0763 D loss: -0.9449 G loss: -0.879\n",
      "Epoch: 0764 D loss: -0.8623 G loss: -1.035\n",
      "Epoch: 0765 D loss: -0.9199 G loss: -0.972\n",
      "Epoch: 0766 D loss: -0.8669 G loss: -1.05\n",
      "Epoch: 0767 D loss: -0.9201 G loss: -0.9261\n",
      "Epoch: 0768 D loss: -0.9869 G loss: -0.8212\n",
      "Epoch: 0769 D loss: -0.9464 G loss: -0.8654\n",
      "Epoch: 0770 D loss: -0.76 G loss: -1.146\n",
      "Epoch: 0771 D loss: -0.9741 G loss: -0.7826\n",
      "Epoch: 0772 D loss: -0.9753 G loss: -0.8219\n",
      "Epoch: 0773 D loss: -0.9116 G loss: -0.9487\n",
      "Epoch: 0774 D loss: -0.9602 G loss: -0.9179\n",
      "Epoch: 0775 D loss: -0.9888 G loss: -0.9042\n",
      "Epoch: 0776 D loss: -0.9174 G loss: -0.9997\n",
      "Epoch: 0777 D loss: -0.742 G loss: -1.288\n",
      "Epoch: 0778 D loss: -0.8525 G loss: -0.9249\n",
      "Epoch: 0779 D loss: -0.9505 G loss: -0.7658\n",
      "Epoch: 0780 D loss: -0.9403 G loss: -0.8099\n",
      "Epoch: 0781 D loss: -0.8112 G loss: -1.069\n",
      "Epoch: 0782 D loss: -0.9832 G loss: -0.8663\n",
      "Epoch: 0783 D loss: -0.8951 G loss: -1.045\n",
      "Epoch: 0784 D loss: -1.03 G loss: -0.8877\n",
      "Epoch: 0785 D loss: -0.984 G loss: -0.9735\n",
      "Epoch: 0786 D loss: -0.8205 G loss: -1.207\n",
      "Epoch: 0787 D loss: -0.9343 G loss: -0.8741\n",
      "Epoch: 0788 D loss: -0.6592 G loss: -1.269\n",
      "Epoch: 0789 D loss: -1.093 G loss: -0.598\n",
      "Epoch: 0790 D loss: -1.012 G loss: -0.7345\n",
      "Epoch: 0791 D loss: -1.153 G loss: -0.7165\n",
      "Epoch: 0792 D loss: -1.094 G loss: -0.8931\n",
      "Epoch: 0793 D loss: -1.078 G loss: -1.028\n",
      "Epoch: 0794 D loss: -1.103 G loss: -1.023\n",
      "Epoch: 0795 D loss: -1.042 G loss: -1.001\n",
      "Epoch: 0796 D loss: -0.7483 G loss: -1.227\n",
      "Epoch: 0797 D loss: -0.9107 G loss: -0.6829\n",
      "Epoch: 0798 D loss: -1.115 G loss: -0.5501\n",
      "Epoch: 0799 D loss: -1.167 G loss: -0.6273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0800 D loss: -1.163 G loss: -0.7714\n",
      "Epoch: 0801 D loss: -1.053 G loss: -1.046\n",
      "Epoch: 0802 D loss: -0.7249 G loss: -2.378\n",
      "Epoch: 0803 D loss: -1.014 G loss: -0.9993\n",
      "Epoch: 0804 D loss: -0.6337 G loss: -1.738\n",
      "Epoch: 0805 D loss: -1.074 G loss: -0.5308\n",
      "Epoch: 0806 D loss: -1.097 G loss: -0.5673\n",
      "Epoch: 0807 D loss: -1.104 G loss: -0.6797\n",
      "Epoch: 0808 D loss: -0.7827 G loss: -1.22\n",
      "Epoch: 0809 D loss: -1.085 G loss: -0.9012\n",
      "Epoch: 0810 D loss: -0.7758 G loss: -1.718\n",
      "Epoch: 0811 D loss: -1.029 G loss: -0.961\n",
      "Epoch: 0812 D loss: -0.922 G loss: -1.017\n",
      "Epoch: 0813 D loss: -1.039 G loss: -0.7009\n",
      "Epoch: 0814 D loss: -1.096 G loss: -0.6534\n",
      "Epoch: 0815 D loss: -0.7708 G loss: -1.064\n",
      "Epoch: 0816 D loss: -1.043 G loss: -0.7307\n",
      "Epoch: 0817 D loss: -0.7657 G loss: -1.12\n",
      "Epoch: 0818 D loss: -0.9467 G loss: -0.896\n",
      "Epoch: 0819 D loss: -1.039 G loss: -0.8569\n",
      "Epoch: 0820 D loss: -0.8106 G loss: -1.262\n",
      "Epoch: 0821 D loss: -1.012 G loss: -0.88\n",
      "Epoch: 0822 D loss: -0.8947 G loss: -1.005\n",
      "Epoch: 0823 D loss: -0.7741 G loss: -1.134\n",
      "Epoch: 0824 D loss: -0.8784 G loss: -0.876\n",
      "Epoch: 0825 D loss: -0.68 G loss: -1.179\n",
      "Epoch: 0826 D loss: -0.9554 G loss: -0.7619\n",
      "Epoch: 0827 D loss: -1.155 G loss: -0.6769\n",
      "Epoch: 0828 D loss: -0.9891 G loss: -0.9502\n",
      "Epoch: 0829 D loss: -1.129 G loss: -0.8763\n",
      "Epoch: 0830 D loss: -1.075 G loss: -1.008\n",
      "Epoch: 0831 D loss: -1.066 G loss: -1.001\n",
      "Epoch: 0832 D loss: -1.084 G loss: -0.8474\n",
      "Epoch: 0833 D loss: -0.9177 G loss: -0.9417\n",
      "Epoch: 0834 D loss: -1.142 G loss: -0.6026\n",
      "Epoch: 0835 D loss: -1.102 G loss: -0.6889\n",
      "Epoch: 0836 D loss: -1.084 G loss: -0.7666\n",
      "Epoch: 0837 D loss: -1.04 G loss: -0.931\n",
      "Epoch: 0838 D loss: -0.8725 G loss: -1.249\n",
      "Epoch: 0839 D loss: -0.9972 G loss: -0.9513\n",
      "Epoch: 0840 D loss: -1.051 G loss: -0.8023\n",
      "Epoch: 0841 D loss: -1.126 G loss: -0.7008\n",
      "Epoch: 0842 D loss: -1.037 G loss: -0.7961\n",
      "Epoch: 0843 D loss: -1.059 G loss: -0.7955\n",
      "Epoch: 0844 D loss: -1.073 G loss: -0.8111\n",
      "Epoch: 0845 D loss: -0.9322 G loss: -1.014\n",
      "Epoch: 0846 D loss: -0.675 G loss: -1.526\n",
      "Epoch: 0847 D loss: -0.7492 G loss: -1.042\n",
      "Epoch: 0848 D loss: -1.135 G loss: -0.5974\n",
      "Epoch: 0849 D loss: -1.074 G loss: -0.752\n",
      "Epoch: 0850 D loss: -1.013 G loss: -0.9395\n",
      "Epoch: 0851 D loss: -0.9877 G loss: -1.067\n",
      "Epoch: 0852 D loss: -0.8785 G loss: -1.284\n",
      "Epoch: 0853 D loss: -1.107 G loss: -0.7779\n",
      "Epoch: 0854 D loss: -0.8466 G loss: -1.024\n",
      "Epoch: 0855 D loss: -1.084 G loss: -0.6648\n",
      "Epoch: 0856 D loss: -1.054 G loss: -0.7478\n",
      "Epoch: 0857 D loss: -1.058 G loss: -0.8223\n",
      "Epoch: 0858 D loss: -0.8256 G loss: -1.223\n",
      "Epoch: 0859 D loss: -0.895 G loss: -1.052\n",
      "Epoch: 0860 D loss: -1.008 G loss: -0.8347\n",
      "Epoch: 0861 D loss: -0.8618 G loss: -0.9979\n",
      "Epoch: 0862 D loss: -0.8933 G loss: -0.9123\n",
      "Epoch: 0863 D loss: -1.038 G loss: -0.7629\n",
      "Epoch: 0864 D loss: -0.7075 G loss: -1.272\n",
      "Epoch: 0865 D loss: -0.9007 G loss: -0.92\n",
      "Epoch: 0866 D loss: -0.7647 G loss: -1.125\n",
      "Epoch: 0867 D loss: -0.989 G loss: -0.8249\n",
      "Epoch: 0868 D loss: -0.5406 G loss: -1.814\n",
      "Epoch: 0869 D loss: -0.8979 G loss: -0.8454\n",
      "Epoch: 0870 D loss: -0.6244 G loss: -1.33\n",
      "Epoch: 0871 D loss: -1.095 G loss: -0.6955\n",
      "Epoch: 0872 D loss: -0.9542 G loss: -0.9692\n",
      "Epoch: 0873 D loss: -1.01 G loss: -0.982\n",
      "Epoch: 0874 D loss: -0.8344 G loss: -1.363\n",
      "Epoch: 0875 D loss: -1.046 G loss: -0.8354\n",
      "Epoch: 0876 D loss: -1.011 G loss: -0.8058\n",
      "Epoch: 0877 D loss: -1.084 G loss: -0.7118\n",
      "Epoch: 0878 D loss: -0.8028 G loss: -1.07\n",
      "Epoch: 0879 D loss: -1.041 G loss: -0.7822\n",
      "Epoch: 0880 D loss: -0.5671 G loss: -1.749\n",
      "Epoch: 0881 D loss: -0.3795 G loss: -2.886\n",
      "Epoch: 0882 D loss: -1.134 G loss: -0.5196\n",
      "Epoch: 0883 D loss: -1.217 G loss: -0.606\n",
      "Epoch: 0884 D loss: -0.9678 G loss: -1.003\n",
      "Epoch: 0885 D loss: -0.813 G loss: -1.451\n",
      "Epoch: 0886 D loss: -0.6883 G loss: -2.237\n",
      "Epoch: 0887 D loss: -1.058 G loss: -0.7609\n",
      "Epoch: 0888 D loss: -1.041 G loss: -0.7235\n",
      "Epoch: 0889 D loss: -0.8386 G loss: -0.9491\n",
      "Epoch: 0890 D loss: -0.9469 G loss: -0.839\n",
      "Epoch: 0891 D loss: -1.05 G loss: -0.8131\n",
      "Epoch: 0892 D loss: -0.8953 G loss: -1.114\n",
      "Epoch: 0893 D loss: -0.9975 G loss: -0.9971\n",
      "Epoch: 0894 D loss: -0.9114 G loss: -1.106\n",
      "Epoch: 0895 D loss: -0.8637 G loss: -1.029\n",
      "Epoch: 0896 D loss: -0.8911 G loss: -0.8485\n",
      "Epoch: 0897 D loss: -1.073 G loss: -0.6769\n",
      "Epoch: 0898 D loss: -1.026 G loss: -0.7905\n",
      "Epoch: 0899 D loss: -1.026 G loss: -0.8879\n",
      "Epoch: 0900 D loss: -0.8796 G loss: -1.178\n",
      "Epoch: 0901 D loss: -0.9648 G loss: -1.034\n",
      "Epoch: 0902 D loss: -1.068 G loss: -0.8161\n",
      "Epoch: 0903 D loss: -1.017 G loss: -0.8478\n",
      "Epoch: 0904 D loss: -0.8342 G loss: -1.003\n",
      "Epoch: 0905 D loss: -1.015 G loss: -0.7694\n",
      "Epoch: 0906 D loss: -0.7166 G loss: -1.181\n",
      "Epoch: 0907 D loss: -0.8763 G loss: -0.9261\n",
      "Epoch: 0908 D loss: -0.9782 G loss: -0.8553\n",
      "Epoch: 0909 D loss: -1.032 G loss: -0.8825\n",
      "Epoch: 0910 D loss: -0.9238 G loss: -1.114\n",
      "Epoch: 0911 D loss: -0.9133 G loss: -1.138\n",
      "Epoch: 0912 D loss: -0.7808 G loss: -1.276\n",
      "Epoch: 0913 D loss: -0.9304 G loss: -0.8261\n",
      "Epoch: 0914 D loss: -0.3977 G loss: -2.351\n",
      "Epoch: 0915 D loss: -1.096 G loss: -0.5502\n",
      "Epoch: 0916 D loss: -0.5371 G loss: -1.342\n",
      "Epoch: 0917 D loss: -0.7562 G loss: -1.054\n",
      "Epoch: 0918 D loss: -0.9608 G loss: -0.9642\n",
      "Epoch: 0919 D loss: -0.9973 G loss: -1.106\n",
      "Epoch: 0920 D loss: -0.6383 G loss: -3.418\n",
      "Epoch: 0921 D loss: -0.886 G loss: -0.9978\n",
      "Epoch: 0922 D loss: -1.06 G loss: -0.6359\n",
      "Epoch: 0923 D loss: -1.111 G loss: -0.6432\n",
      "Epoch: 0924 D loss: -0.5836 G loss: -1.431\n",
      "Epoch: 0925 D loss: -1.089 G loss: -0.7262\n",
      "Epoch: 0926 D loss: -0.6192 G loss: -1.506\n",
      "Epoch: 0927 D loss: -0.4535 G loss: -2.212\n",
      "Epoch: 0928 D loss: -0.8163 G loss: -0.8449\n",
      "Epoch: 0929 D loss: -0.7979 G loss: -0.9361\n",
      "Epoch: 0930 D loss: -0.8602 G loss: -0.9828\n",
      "Epoch: 0931 D loss: -1.068 G loss: -0.8908\n",
      "Epoch: 0932 D loss: -1.02 G loss: -1.117\n",
      "Epoch: 0933 D loss: -0.9808 G loss: -1.254\n",
      "Epoch: 0934 D loss: -0.7513 G loss: -1.642\n",
      "Epoch: 0935 D loss: -1.246 G loss: -0.4818\n",
      "Epoch: 0936 D loss: -1.233 G loss: -0.5524\n",
      "Epoch: 0937 D loss: -1.2 G loss: -0.707\n",
      "Epoch: 0938 D loss: -0.8136 G loss: -1.295\n",
      "Epoch: 0939 D loss: -0.8348 G loss: -1.313\n",
      "Epoch: 0940 D loss: -0.6918 G loss: -1.509\n",
      "Epoch: 0941 D loss: -0.8331 G loss: -0.8477\n",
      "Epoch: 0942 D loss: -0.6657 G loss: -1.029\n",
      "Epoch: 0943 D loss: -1.293 G loss: -0.5193\n",
      "Epoch: 0944 D loss: -0.7107 G loss: -1.282\n",
      "Epoch: 0945 D loss: -1.215 G loss: -0.8122\n",
      "Epoch: 0946 D loss: -1.164 G loss: -1.033\n",
      "Epoch: 0947 D loss: -1.013 G loss: -1.39\n",
      "Epoch: 0948 D loss: -0.9563 G loss: -1.061\n",
      "Epoch: 0949 D loss: -1.012 G loss: -0.6602\n",
      "Epoch: 0950 D loss: -0.6833 G loss: -0.9453\n",
      "Epoch: 0951 D loss: -0.8433 G loss: -0.7676\n",
      "Epoch: 0952 D loss: -1.017 G loss: -0.7745\n",
      "Epoch: 0953 D loss: -0.6176 G loss: -1.508\n",
      "Epoch: 0954 D loss: -0.8256 G loss: -1.233\n",
      "Epoch: 0955 D loss: -0.7092 G loss: -1.616\n",
      "Epoch: 0956 D loss: -0.7224 G loss: -1.353\n",
      "Epoch: 0957 D loss: -1.187 G loss: -0.5822\n",
      "Epoch: 0958 D loss: -0.9999 G loss: -0.8047\n",
      "Epoch: 0959 D loss: -0.8496 G loss: -1.05\n",
      "Epoch: 0960 D loss: -1.11 G loss: -0.7997\n",
      "Epoch: 0961 D loss: -0.8388 G loss: -1.224\n",
      "Epoch: 0962 D loss: -0.7645 G loss: -1.257\n",
      "Epoch: 0963 D loss: -0.7868 G loss: -0.981\n",
      "Epoch: 0964 D loss: -1.012 G loss: -0.6998\n",
      "Epoch: 0965 D loss: -0.8713 G loss: -0.9382\n",
      "Epoch: 0966 D loss: -1.489 G loss: -0.5663\n",
      "Epoch: 0967 D loss: -0.7599 G loss: -1.623\n",
      "Epoch: 0968 D loss: -0.654 G loss: -2.087\n",
      "Epoch: 0969 D loss: -1.239 G loss: -0.5396\n",
      "Epoch: 0970 D loss: -0.5245 G loss: -1.547\n",
      "Epoch: 0971 D loss: -0.9548 G loss: -0.6931\n",
      "Epoch: 0972 D loss: -0.8939 G loss: -0.8838\n",
      "Epoch: 0973 D loss: -1.231 G loss: -0.7496\n",
      "Epoch: 0974 D loss: -0.7236 G loss: -1.721\n",
      "Epoch: 0975 D loss: -1.294 G loss: -0.7726\n",
      "Epoch: 0976 D loss: -0.7733 G loss: -1.829\n",
      "Epoch: 0977 D loss: -1.077 G loss: -0.6689\n",
      "Epoch: 0978 D loss: -0.9177 G loss: -0.7745\n",
      "Epoch: 0979 D loss: -1.305 G loss: -0.5509\n",
      "Epoch: 0980 D loss: -1.527 G loss: -0.5688\n",
      "Epoch: 0981 D loss: -0.6545 G loss: -2.307\n",
      "Epoch: 0982 D loss: -1.259 G loss: -0.7032\n",
      "Epoch: 0983 D loss: -1.363 G loss: -0.6232\n",
      "Epoch: 0984 D loss: -0.8881 G loss: -1.117\n",
      "Epoch: 0985 D loss: -0.6415 G loss: -1.368\n",
      "Epoch: 0986 D loss: -0.7651 G loss: -0.8252\n",
      "Epoch: 0987 D loss: -1.497 G loss: -0.4262\n",
      "Epoch: 0988 D loss: -0.9485 G loss: -0.9987\n",
      "Epoch: 0989 D loss: -1.142 G loss: -0.9833\n",
      "Epoch: 0990 D loss: -0.8183 G loss: -2.004\n",
      "Epoch: 0991 D loss: -1.204 G loss: -0.7937\n",
      "Epoch: 0992 D loss: -1.117 G loss: -0.7237\n",
      "Epoch: 0993 D loss: -1.358 G loss: -0.5119\n",
      "Epoch: 0994 D loss: -0.8239 G loss: -1.029\n",
      "Epoch: 0995 D loss: -1.262 G loss: -0.6207\n",
      "Epoch: 0996 D loss: -1.008 G loss: -0.9503\n",
      "Epoch: 0997 D loss: -0.8752 G loss: -1.186\n",
      "Epoch: 0998 D loss: -0.7582 G loss: -1.309\n",
      "Epoch: 0999 D loss: -0.4011 G loss: -3.402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# 신경망 모델 학습\n",
    "#########\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "loss_val_D, loss_val_G = 0, 0\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    noise = get_noise(batch_size, n_noise)\n",
    "    # 판별기와 생성기 신경망을 각각 학습시킵니다.\n",
    "    _, loss_val_D = sess.run([train_D, loss_D],feed_dict={X: pix, Z: noise})\n",
    "    _, loss_val_G = sess.run([train_G, loss_G],feed_dict={Z: noise})\n",
    "\n",
    "    print('Epoch:', '%04d' % epoch,\n",
    "          'D loss: {:.4}'.format(loss_val_D),\n",
    "          'G loss: {:.4}'.format(loss_val_G))\n",
    "     #########\n",
    "    # 학습이 되어가는 모습을 보기 위해 주기적으로 이미지를 생성하여 저장\n",
    "    ######\n",
    "    if epoch == 0 or (epoch + 1) % 100 == 0:\n",
    "        sample_size = 1\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G, feed_dict={Z: noise})\n",
    "        samples = np.reshape(samples, (64, 64, 3))\n",
    "        fig, ax = plt.subplots(1, sample_size, figsize=(10, 10))\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(samples)\n",
    "        plt.savefig('samples/{}.png'.format(str(epoch+1).zfill(3)), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "print('최적화 완료!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
